{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images - line by line, cmaera - col by col\n",
    "    Q: is this hardware specific?\n",
    "screen waits 1 frame cycle, camera waits 1 frame cycle \n",
    "    Q: r they the same?\n",
    "\n",
    "\n",
    "two kinds of challenges:\n",
    "    background challenge: one color\n",
    "    lighting challenge: belt of different colr from background color\n",
    "        belt is \"lighting area\"\n",
    "\n",
    "ROI: region that camera is scanning when the screen is displaying the lighting area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from sklearn.linear_model import LinearRegression as lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = \"videoframes_raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eqn 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eqn_2(pixel, color1, color2, E):\n",
    "    '''\n",
    "    #INPUTS:\n",
    "        pixel: a single pixel with all 3 color channels\n",
    "        color1: background color being shown on screen\n",
    "        color2: primary color being shown on screen (band)\n",
    "        E: illuminance for all 3 channels \n",
    "\n",
    "    Confirm that I{c1}/I{c2} = E{c1}/E{c2} (where c1 and c2 are the 2 colors being shown on the screen)  \n",
    "    '''\n",
    "\n",
    "    iFraction = pixel[color1]/(pixel[color2]+1)\n",
    "    eFraction = E[color1]/E[color2]\n",
    "    epsilon = 0.05\n",
    "    return iFraction- eFraction <= epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1171722412109375\n"
     ]
    }
   ],
   "source": [
    "#TESTING running eqn. 2\n",
    "image = cv2.imread('videoframes_cropped/frame_0002.jpg')\n",
    "\n",
    "color1 = 0\n",
    "color2 = 1\n",
    "E = [234,251,0]\n",
    "count = 0\n",
    "# Apply Eqn 2 on every pixel between response of lighting challenge and background challenge\n",
    "for r in range(image.shape[0]):\n",
    "    for c in range(image.shape[1]):\n",
    "        consistent = eqn_2(image[r][c][:], color1, color2, E)\n",
    "        if not consistent:\n",
    "            count +=1\n",
    "\n",
    "print(count/(image.shape[0]*image.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 30 #(camera)\n",
    "# 1/120 ms to draw one frame on the screen\n",
    "\n",
    "def roi(t_u, u, rows, cols, ct_k, ct_frame, fps, color1, color2, E):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        t_u = time that this color started\n",
    "        u = top of band\n",
    "        rows = # of rows contained in one frame\n",
    "        cols = number of columns contained in one captured frame \n",
    "        ct_k = start time to exposure the first column of k-th capture frame\n",
    "        ct_frame = exposure time of one captured frame\n",
    "        fps = frame rate of the camera taking the video\n",
    "        color1 = background color\n",
    "        color2 = band color\n",
    "    '''\n",
    "\n",
    "    # Apply Eqn 2 on every pixel between response of lighting challenge and background challenge\n",
    "    for r in range(image.shape[0]):\n",
    "        for c in range(image.shape[1]):\n",
    "            consistent = eqn_2(image[r][c][:], color1, color2, E)\n",
    "            if not consistent:\n",
    "                print(\"Not consistent!\")\n",
    "                return\n",
    "    \n",
    "    # CALCULATE ROI \n",
    "    path = 'videoframes_cropped/frame_' + (fps*t_u) + '.jpg'\n",
    "    k =  cv2.imread() #first captured frame whose recording period covers t_u\n",
    "    a = cols * (t_u - ct_k)/ct_frame\n",
    "    b = a+0.25*k.shape[1]\n",
    "\n",
    "    # regression model\n",
    "    a,b=10,50\n",
    "    ROI = (a,b)\n",
    "    \n",
    "    imgWidth = image.shape[1]\n",
    "    y_hat = 0\n",
    "    if a < 0.25*imgWidth:\n",
    "        avgCols = None # take average value of columns [a,25]\n",
    "        output = None #lr1.predict(avgCols) #run regression on model 1\n",
    "        y_hat += (0.25*imgWidth-a)*output\n",
    "    if b < 0.5*imgWidth:\n",
    "        #model2: avg. value of columns [25, b]\n",
    "        # run regression on model 2 \n",
    "        avgCols = None\n",
    "        output = None #lr2.predict(avgCols)\n",
    "        y_hat += (b-0.25*imgWidth)*output\n",
    "\n",
    "    if b < 0.75*imgWidth and a > 0.25*imgWidth:\n",
    "        #model 3: avg value of columns [50,b]\n",
    "        avgCols = None\n",
    "        output = None #lr3.predict(avgCols)\n",
    "        y_hat += (b-0.5*imgWidth)*output\n",
    "\n",
    "    if b >=0.75*imgWidth:\n",
    "        #model 4 : avg value of columns [75,b]\n",
    "        avgCols = None\n",
    "        output = None #lr4.predict(avgCols)\n",
    "        y_hat += (b-0.75*imgWidth)*output\n",
    "    \n",
    "    y_hat = y_hat /imgWidth\n",
    "\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Calculate ROI\n",
    "# calculate start time to show the lighting area, t_u\n",
    "t_u = 0 #(u / rows) * t_frame\n",
    "# find image - frame at t_u\n",
    "img = frames[0] #[fps * t_u]\n",
    "\n",
    "# calculate shift, l\n",
    "l = 0 #cols * (t_u - ct_k) / (ct_frame)\n",
    "\n",
    "u = 0.75 * img.shape[0]\n",
    "rows = 0\n",
    "t_frame = 0\n",
    "cols = 0\n",
    "ct_k = 0\n",
    "ct_frame = 0\n",
    "fps = 0\n",
    "color1 = 0\n",
    "color2 = 1\n",
    "E = [234,251,0]\n",
    "\n",
    "y_hat = roi(u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criteria(frames):\n",
    "    t_begin, u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E = 1,2,3,4,5,6,7,8,9,10,11\n",
    "    \n",
    "    dVals = []\n",
    "    for frame in frames:\n",
    "        y_hat_i = roi(frames, t_begin, u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E)\n",
    "\n",
    "        # calc criteria\n",
    "        d = 0.25 #% of screen the band is shown on \n",
    "        imgRows = frame.shape[0] \n",
    "        \n",
    "        d_i = y_hat_i - (u + u+imgRows*0.25)/2\n",
    "        dVals.append(d_i)\n",
    "\n",
    "    mean = np.mean(dVals)\n",
    "    var = np.var(dVals)\n",
    "\n",
    "    threshold = -5\n",
    "    return mean * np.sqrt(var) < np.exp(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training linear regression models to be used to identify location of band of color\n",
    "#TO DO: need data to train on !! \n",
    "lr1 = lr().fit(x1_train, y1_train)\n",
    "lr2 = lr().fit(x2_train, y2_train)\n",
    "lr3 = lr().fit(x3_train, y3_train)\n",
    "lr4 = lr().fit(x4_train, y4_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
