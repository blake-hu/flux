{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images - line by line, cmaera - col by col\n",
    "    Q: is this hardware specific?\n",
    "screen waits 1 frame cycle, camera waits 1 frame cycle \n",
    "    Q: r they the same?\n",
    "\n",
    "\n",
    "two kinds of challenges:\n",
    "    background challenge: one color\n",
    "    lighting challenge: belt of different colr from background color\n",
    "        belt is \"lighting area\"\n",
    "\n",
    "ROI: region that camera is scanning when the screen is displaying the lighting area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from sklearn.linear_model import LinearRegression as lr \n",
    "import pandas as pd\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = \"blake_video.mp4\" # Replace with path of input video\n",
    "output_directory1 = \"output_1\"\n",
    "frames = \"videoframes_raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split video into frames in the videoframes directory\n",
    "# Runtime: ~60 seconds\n",
    "\n",
    "# Delete directory if it already exists and make a new one\n",
    "if os.path.exists(output_directory1):\n",
    "   shutil.rmtree(output_directory1)\n",
    "os.makedirs(output_directory1)\n",
    "\n",
    "vidcap = cv2.VideoCapture(input_video)\n",
    "fps = vidcap.get(cv2.CAP_PROP_FPS) # get fps\n",
    "frametime_list = []\n",
    "print(\"FPS\", fps)\n",
    "\n",
    "if vidcap.isOpened():\n",
    "  frame_count = 0\n",
    "\n",
    "  while True:\n",
    "      ret, frame = vidcap.read()\n",
    "      \n",
    "      if not ret:\n",
    "          break\n",
    "      \n",
    "      # Get the timestamp of the current frame in milliseconds -> TODO: currently unused \n",
    "      timestamp = vidcap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "      frametime_list.append(timestamp)\n",
    "      \n",
    "      # Save each frame as an image in the output folder\n",
    "      frame_name = f\"frame_{frame_count:04d}.jpg\"\n",
    "      frame_path = os.path.join(output_directory1, frame_name)\n",
    "      cv2.imwrite(frame_path, frame)\n",
    "\n",
    "      frame_count += 1\n",
    "\n",
    "  vidcap.release()\n",
    "\n",
    "  frametime_list = np.array(frametime_list)\n",
    "\n",
    "  print(f\"Video frames saved in '{output_directory1}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT FRAMES INTO CROPPED FRAMES \n",
    "if os.path.exists(frames):\n",
    "   shutil.rmtree(frames)\n",
    "os.makedirs(frames)\n",
    "\n",
    "for filename in sorted(os.listdir(output_directory1)):\n",
    "    frame_path = os.path.join(output_directory1, filename)\n",
    "    \n",
    "    if os.path.isfile(frame_path):  # Check if it's a file (not a subdirectory)\n",
    "        #face detection and alignment\n",
    "        img = cv2.imread(frame_path)\n",
    "\n",
    "        try: #skip frame if can't detect face\n",
    "            face_objs = DeepFace.extract_faces(img_path = frame_path, \n",
    "                    target_size = (224, 224)\n",
    "            )\n",
    "        except:\n",
    "            print(\"Couldn't detect face in frame \" + frame_path)\n",
    "\n",
    "        face = face_objs[0]['facial_area']\n",
    "        x,y,w,h = face['x'],face['y'],face['w'],face['h']\n",
    "\n",
    "        img = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Save each frame as an image in the output folder\n",
    "        frame_name = f\"frame_{frame_count:04d}.jpg\"\n",
    "        frame_path = os.path.join(frames, filename)\n",
    "        cv2.imwrite(frame_path, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eqn 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eqn_2(pixel, color1, color2, E):\n",
    "    '''\n",
    "    #INPUTS:\n",
    "        pixel: a single pixel with all 3 color channels\n",
    "        color1: background color being shown on screen\n",
    "        color2: primary color being shown on screen (band)\n",
    "        E: illuminance for all 3 channels \n",
    "\n",
    "    Confirm that I{c1}/I{c2} = E{c1}/E{c2} (where c1 and c2 are the 2 colors being shown on the screen)  \n",
    "    '''\n",
    "    iFraction = pixel[color1]/(pixel[color2]+1)\n",
    "    eFraction = E[color1]/E[color2]\n",
    "    epsilon = 0.05\n",
    "    return iFraction- eFraction <= epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyEqn2(color1, color2, E, img):\n",
    "    # Apply Eqn 2 on every pixel between response of lighting challenge and background challenge\n",
    "    count = 0\n",
    "    for r in range(img.shape[0]):\n",
    "        for c in range(img.shape[1]):\n",
    "            consistent = eqn_2(img[r][c][:], color1, color2, E)\n",
    "            if not consistent:\n",
    "                count+=1\n",
    "                # print(\"Not consistent!\")\n",
    "                # return\n",
    "\n",
    "    return (count/(img.shape[0]*img.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.306488037109375\n"
     ]
    }
   ],
   "source": [
    "#EQN2 CHECK:\n",
    "image = cv2.imread('videoframes_cropped/frame_0002.jpg')\n",
    "color1 = 0\n",
    "color2 = 1\n",
    "E = [234,251,0]\n",
    "count = 0\n",
    "inconsistent = verifyEqn2(color1, color2, E, image)\n",
    "print(inconsistent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colorChanges = pd.read_csv('backend.csv')\n",
    "frameTimes = []\n",
    "\n",
    "for i in colorChanges: #for each color change\n",
    "    color1, color2, u, startTime \n",
    "    offset = # randomly select an offset \n",
    "    t_u = startTime+offset\n",
    "    imgIndex = np.sum(frameTimes<t_u)-1 #find all frames capturing time less than t_u, select last one\n",
    "    imgPath = ' ' + frameTimes[imgIndex]\n",
    "    img = cv2.imread(imgPath)\n",
    "    exposureTime =  frameTimes[imgIndex+1]-frameTimes[imgIndex]\n",
    "\n",
    "    #calculate eqn2 \n",
    "    verifyEqn2(color1, color2, E, img)\n",
    "    #STOP HERE IF DOESNT PASS\n",
    "\n",
    "    a,b = roi(t_u, startTime, exposureTime, img)\n",
    "    #find the corresponding cv2 image that covers that time frame\n",
    "\n",
    "\n",
    "fps = 30 #(camera)\n",
    "# 1/120 ms to draw one frame on the screen\n",
    "\"\"\" \n",
    "starting w/ frontend's csv, select a random time t_u, \n",
    "input t_u, ct_k which is the start time of that color, \n",
    "check what colors are appearing at that time, input that into ROI \n",
    "\"\"\"\n",
    "def roi(t_u, ct_k, ct_frame, image):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        t_u = time that this color started\n",
    "        u = top of band\n",
    "        ct_k = start time to exposure the first column of k-th capture frame\n",
    "            --> find w/ firstImg\n",
    "        ct_frame = exposure time of one captured frame \n",
    "            --> average time of each frame ? maybe can calculate w/ dict. \n",
    "        image = first image whose recording period covers t_u\n",
    "    '''\n",
    "    cols = image.shape[1]\n",
    "    a = cols * (t_u - ct_k)/ct_frame\n",
    "    b = a+0.2*image.shape[1]\n",
    "    return [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.066666666666666, 68.26666666666667]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING ROI\n",
    "image = cv2.imread('videoframes_cropped/frame_0002.jpg')\n",
    "t_u = 3\n",
    "ct_k = 1\n",
    "ct_frame = 30\n",
    "roi(t_u, ct_k, ct_frame, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models():\n",
    "    # regression model\n",
    "    a,b=10,50\n",
    "    ROI = (a,b)\n",
    "    \n",
    "    imgWidth = image.shape[1]\n",
    "\n",
    "    y_hat = 0\n",
    "    if a < 0.25*imgWidth:\n",
    "        avgCols = np.mean(image[:, a:25, :]) # take average value of columns [a,25] \n",
    "                ## ACROSS RGB ??\n",
    "        output = lr1.predict(avgCols) #run regression on model 1\n",
    "        y_hat += (0.25*imgWidth-a)*output\n",
    "    if b < 0.5*imgWidth:\n",
    "        #model2: avg. value of columns [25, b]\n",
    "        # run regression on model 2 \n",
    "        avgCols = None \n",
    "        output = lr2.predict(avgCols)\n",
    "        y_hat += (b-0.25*imgWidth)*output\n",
    "\n",
    "    if b < 0.75*imgWidth and a > 0.25*imgWidth:\n",
    "        #model 3: avg value of columns [50,b]\n",
    "        avgCols = None\n",
    "        output = lr3.predict(avgCols)\n",
    "        y_hat += (b-0.5*imgWidth)*output\n",
    "\n",
    "    if b >=0.75*imgWidth:\n",
    "        #model 4 : avg value of columns [75,b]\n",
    "        avgCols = None\n",
    "        output = lr4.predict(avgCols)\n",
    "        y_hat += (b-0.75*imgWidth)*output\n",
    "    \n",
    "    y_hat = y_hat /imgWidth\n",
    "\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Calculate ROI\n",
    "# calculate start time to show the lighting area, t_u\n",
    "t_u = 0 #(u / rows) * t_frame\n",
    "# find image - frame at t_u\n",
    "img = frames[0] #[fps * t_u]\n",
    "\n",
    "# calculate shift, l\n",
    "l = 0 #cols * (t_u - ct_k) / (ct_frame)\n",
    "\n",
    "u = 0.75 * img.shape[0]\n",
    "rows = 0\n",
    "t_frame = 0\n",
    "cols = 0\n",
    "ct_k = 0\n",
    "ct_frame = 0\n",
    "fps = 0\n",
    "color1 = 0\n",
    "color2 = 1\n",
    "E = [234,251,0]\n",
    "\n",
    "y_hat = roi(u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criteria(frames):\n",
    "    t_begin, u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E = 1,2,3,4,5,6,7,8,9,10,11\n",
    "    \n",
    "    dVals = []\n",
    "    for frame in frames:\n",
    "        y_hat_i = roi(frames, t_begin, u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E)\n",
    "\n",
    "        # calc criteria\n",
    "        d = 0.25 #% of screen the band is shown on \n",
    "        imgRows = frame.shape[0] \n",
    "        \n",
    "        d_i = y_hat_i - (u + u+imgRows*0.25)/2\n",
    "        dVals.append(d_i)\n",
    "\n",
    "    mean = np.mean(dVals)\n",
    "    var = np.var(dVals)\n",
    "\n",
    "    threshold = -5\n",
    "    return mean * np.sqrt(var) < np.exp(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training linear regression models to be used to identify location of band of color\n",
    "#TO DO: need data to train on !! \n",
    "lr1 = lr().fit(x1_train, y1_train)\n",
    "lr2 = lr().fit(x2_train, y2_train)\n",
    "lr3 = lr().fit(x3_train, y3_train)\n",
    "lr4 = lr().fit(x4_train, y4_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
