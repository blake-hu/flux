{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images - line by line, cmaera - col by col\n",
    "    Q: is this hardware specific?\n",
    "screen waits 1 frame cycle, camera waits 1 frame cycle \n",
    "    Q: r they the same?\n",
    "\n",
    "\n",
    "two kinds of challenges:\n",
    "    background challenge: one color\n",
    "    lighting challenge: belt of different colr from background color\n",
    "        belt is \"lighting area\"\n",
    "\n",
    "ROI: region that camera is scanning when the screen is displaying the lighting area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pandas as pd\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = \"shreya.mp4\" # Replace with path of input video\n",
    "output_directory1 = \"output_1\"\n",
    "frames = \"videoframes_raw3\"\n",
    "colorchange_csv = 'shreya.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS 30.007227174174897\n",
      "Video frames saved in 'output_1'\n"
     ]
    }
   ],
   "source": [
    "# Split video into frames in the videoframes directory\n",
    "# Runtime: ~60 seconds\n",
    "\n",
    "# Delete directory if it already exists and make a new one\n",
    "if os.path.exists(output_directory1):\n",
    "   shutil.rmtree(output_directory1)\n",
    "os.makedirs(output_directory1)\n",
    "\n",
    "vidcap = cv2.VideoCapture(input_video)\n",
    "fps = vidcap.get(cv2.CAP_PROP_FPS) # get fps\n",
    "frametime_list = []\n",
    "print(\"FPS\", fps)\n",
    "\n",
    "if vidcap.isOpened():\n",
    "  while True:\n",
    "      ret, frame = vidcap.read()\n",
    "      \n",
    "      if not ret:\n",
    "          break\n",
    "      \n",
    "      # Get the timestamp of the current frame in milliseconds \n",
    "      timestamp = round(vidcap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "      frametime_list.append(timestamp)\n",
    "      \n",
    "      # Save each frame as an image in the output folder\n",
    "      frame_name = f\"frame_{timestamp}.jpg\"\n",
    "      frame_path = os.path.join(output_directory1, frame_name)\n",
    "      cv2.imwrite(frame_path, frame)\n",
    "\n",
    "  vidcap.release()\n",
    "\n",
    "  frametime_list = np.array(frametime_list)\n",
    "\n",
    "  print(f\"Video frames saved in '{output_directory1}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT FRAMES INTO CROPPED FRAMES \n",
    "if os.path.exists(frames):\n",
    "   shutil.rmtree(frames)\n",
    "os.makedirs(frames)\n",
    "\n",
    "for filename in sorted(os.listdir(output_directory1)):\n",
    "    frame_path = os.path.join(output_directory1, filename)\n",
    "    \n",
    "    if os.path.isfile(frame_path):  # Check if it's a file (not a subdirectory)\n",
    "        #face detection and alignment\n",
    "        img = cv2.imread(frame_path)\n",
    "\n",
    "        try: #skip frame if can't detect face\n",
    "            face_objs = DeepFace.extract_faces(img_path = frame_path, \n",
    "                    target_size = (224, 224)\n",
    "            )\n",
    "        except:\n",
    "            print(\"Couldn't detect face in frame \" + frame_path)\n",
    "\n",
    "        face = face_objs[0]['facial_area']\n",
    "        x,y,w,h = face['x'],face['y'],face['w'],face['h']\n",
    "\n",
    "        img = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Save each frame as an image in the output folder\n",
    "        # frame_name = f\"frame_{frame_count:04d}.jpg\"\n",
    "        frame_path = os.path.join(frames, filename)\n",
    "        cv2.imwrite(frame_path, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eqn 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eqn_2(pixel, color1, color2, E):\n",
    "    '''\n",
    "    #INPUTS:\n",
    "        pixel: a single pixel with all 3 color channels\n",
    "        color1: background color being shown on screen\n",
    "        color2: primary color being shown on screen (band)\n",
    "        E: illuminance for all 3 channels \n",
    "\n",
    "    Confirm that I{c1}/I{c2} = E{c1}/E{c2} (where c1 and c2 are the 2 colors being shown on the screen)  \n",
    "    '''\n",
    "    iFraction = pixel[color1]/(pixel[color2]+1)\n",
    "    eFraction = E[color1]/E[color2]\n",
    "    epsilon = 0.01\n",
    "    return iFraction- eFraction <= epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyEqn2(color1, color2, img):\n",
    "    # Apply Eqn 2 on every pixel between response of lighting challenge and background challenge\n",
    "    count = 0\n",
    "    E = [0,0,0]\n",
    "    E[int(color1)] = 256\n",
    "    E[int(color2)] = 256\n",
    "    for r in range(img.shape[0]):\n",
    "        for c in range(img.shape[1]):\n",
    "            consistent = eqn_2(img[r][c][:], color1, color2, E)\n",
    "            if not consistent:\n",
    "                count+=1\n",
    "                # print(\"Not consistent!\")\n",
    "                # return\n",
    "\n",
    "    return (count/(img.shape[0]*img.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09526234567901234\n"
     ]
    }
   ],
   "source": [
    "#EQN2 CHECK:\n",
    "image = cv2.imread('videoframes_raw/frame_0.jpg')\n",
    "color1 = 1\n",
    "color2 = 2\n",
    "E = [0,256,256]\n",
    "inconsistent = verifyEqn2(color1, color2, image)\n",
    "print(inconsistent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 30 #(camera)\n",
    "# 1/120 ms to draw one frame on the screen\n",
    "def roi(t_u, ct_k, ct_frame, image):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        t_u = time that this color started\n",
    "        u = top of band\n",
    "        ct_k = start time to exposure the first column of k-th capture frame\n",
    "            --> find w/ firstImg\n",
    "        ct_frame = exposure time of one captured frame \n",
    "            --> average time of each frame ? maybe can calculate w/ dict. \n",
    "        image = first image whose recording period covers t_u\n",
    "    '''\n",
    "    cols = image.shape[1]\n",
    "    a = cols * (t_u - ct_k)/ct_frame\n",
    "    b = a+0.2*image.shape[1]\n",
    "    return [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Background Color</th>\n",
       "      <th>Strip Color</th>\n",
       "      <th>Strip Position</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Background Color  Strip Color  Strip Position   Timestamp\n",
       "0                0            1             0.2           0\n",
       "1                1            2             0.5         500\n",
       "2                1            0             0.5        1001\n",
       "3                1            2             0.6        1501\n",
       "4                0            1             0.6        2000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapColors(color):\n",
    "    if color == \"Red\":\n",
    "        return 0\n",
    "    if color == \"Green\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def removePercent(s):\n",
    "    return float(\"0.\" + s[:-1])\n",
    "\n",
    "colorChanges = pd.read_csv(colorchange_csv)\n",
    "colorChanges.iloc[:,0] = colorChanges.iloc[:,0].apply(mapColors)\n",
    "colorChanges.iloc[:,1] = colorChanges.iloc[:,1].apply(mapColors)\n",
    "colorChanges.iloc[:,2] = colorChanges.iloc[:,2].apply(removePercent)\n",
    "colorChanges.iloc[:,3]= colorChanges.iloc[:,3]-colorChanges.iloc[0,3]\n",
    "\n",
    "colorChanges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "videoframes_raw2/frame_0.jpg\n",
      "(173, 173, 3)\n",
      "inconsistency:  0.07504427144241371\n",
      "a, b:  16 51\n",
      "videoframes_raw2/frame_498.jpg\n",
      "(173, 173, 3)\n",
      "inconsistency:  0.017875639012329178\n",
      "a, b:  10 45\n",
      "videoframes_raw2/frame_998.jpg\n",
      "(177, 177, 3)\n",
      "inconsistency:  0.8439145839318204\n",
      "INCONSISTENT\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): #for each color change\n",
    "    color1, color2 = colorChanges.iloc[i,0],colorChanges.iloc[i,1]\n",
    "    u, startTime = colorChanges.iloc[i,2], colorChanges.iloc[i,3]\n",
    "    offset = random.randint(2,6)# randomly select a ms offset \n",
    "    t_u = startTime+offset\n",
    "    \n",
    "    imgIndex = np.sum(frametime_list<t_u)-1 #find all frames capturing time less than t_u, select last one\n",
    "    imgPath = f'{frames}/frame_' + str(frametime_list[imgIndex]) + \".jpg\"\n",
    "    print(imgPath)\n",
    "    img = cv2.imread(imgPath)\n",
    "    print(img.shape)\n",
    "\n",
    "    exposureTime =  frametime_list[imgIndex+1]-frametime_list[imgIndex]\n",
    "\n",
    "    #calculate eqn2 \n",
    "    inconsistency = verifyEqn2(color1, color2, img)\n",
    "    print(\"inconsistency: \", inconsistency)\n",
    "    if inconsistency >0.1:     #STOP HERE IF DOESNT PASS\n",
    "        print(\"INCONSISTENT\")\n",
    "        break\n",
    "\n",
    "    a,b = roi(t_u, startTime, exposureTime, img)\n",
    "    a = round(a)\n",
    "    b = round(b)\n",
    "    print(\"a, b: \", a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criteria(frames):\n",
    "    t_begin, u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E = 1,2,3,4,5,6,7,8,9,10,11\n",
    "    \n",
    "    dVals = []\n",
    "    for frame in frames:\n",
    "        y_hat_i = roi(frames, t_begin, u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E)\n",
    "\n",
    "        # calc criteria\n",
    "        d = 0.25 #% of screen the band is shown on \n",
    "        imgRows = frame.shape[0] \n",
    "        \n",
    "        d_i = y_hat_i - (u + u+imgRows*0.25)/2\n",
    "        dVals.append(d_i)\n",
    "\n",
    "    mean = np.mean(dVals)\n",
    "    var = np.var(dVals)\n",
    "\n",
    "    threshold = -5\n",
    "    return mean * np.sqrt(var) < np.exp(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training linear regression models to be used to identify location of band of color\n",
    "def get_average_vector(roi):\n",
    "    # Calculate the mean along the height and width (axis 0 and 1), resulting in mean color\n",
    "    return np.mean(roi, axis=(0, 1))\n",
    "\n",
    "\n",
    "def lr_fit(rois, targets):\n",
    "    # predictions = []\n",
    "    for i in range(len(rois)):\n",
    "        avg_vector = get_average_vector(rois[i])\n",
    "\n",
    "        lr = LinearRegression().fit([avg_vector], [targets[i]])\n",
    "    return lr\n",
    "    # return predictions\n",
    "\n",
    "def lr_predict(lr, rois, targets):\n",
    "    predictions = []\n",
    "    for i in range(len(rois)):\n",
    "        avg_vector = get_average_vector(rois[i])\n",
    "        prediction = lr.predict([avg_vector])\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    score = mse(targets, predictions)\n",
    "    \n",
    "    return predictions, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "\n",
    "def find_closest_filename(folder_path, target):\n",
    "    closest_filename = None\n",
    "    closest_distance = float('inf')  # Initialize with a large number\n",
    "    best_ctk = 0\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.startswith('frame_') and filename.endswith('.jpg'):\n",
    "            # Extract the number from the filename\n",
    "            number_part = filename.replace('frame_', '').replace('.jpg', '')\n",
    "            try:\n",
    "                number = int(number_part)\n",
    "                if number <= target: # tu - ctk >= 30\n",
    "                # if target - number < 24:\n",
    "                    # Calculate the absolute difference from the target\n",
    "                    distance = abs(number - target)\n",
    "\n",
    "                    # Update the closest filename if this file is closer\n",
    "                    if distance < closest_distance:\n",
    "                        closest_distance = distance\n",
    "                        closest_filename = filename\n",
    "                        best_ctk = number\n",
    "            except ValueError:\n",
    "                # Handle the case where conversion to int fails\n",
    "                continue\n",
    "\n",
    "    return closest_filename, best_ctk\n",
    "\n",
    "\n",
    "rois = []\n",
    "targets = []\n",
    "\n",
    "data_files = ['colors.csv', 'shun.csv']\n",
    "data_frames = ['videoframes_raw', 'videoframes_raw2']\n",
    "\n",
    "for j in range(len(data_files)):\n",
    "    colorChanges = pd.read_csv(data_files[j])\n",
    "    colorChanges.iloc[:,0] = colorChanges.iloc[:,0].apply(mapColors)\n",
    "    colorChanges.iloc[:,1] = colorChanges.iloc[:,1].apply(mapColors)\n",
    "    colorChanges.iloc[:,2] = colorChanges.iloc[:,2].apply(removePercent)\n",
    "    colorChanges.iloc[:,3]= colorChanges.iloc[:,3]-colorChanges.iloc[0,3]\n",
    "\n",
    "    for i in range(colorChanges.shape[0] - 1): #for each color change -> TODO: this should b number of lines in csv\n",
    "        color1, color2 = colorChanges.iloc[i,0],colorChanges.iloc[i,1]\n",
    "        u, startTime = colorChanges.iloc[i,2], colorChanges.iloc[i,3]\n",
    "        offset = random.uniform(0,1) * 3 # randomly select a ms offset \n",
    "        t_u = startTime+offset\n",
    "        \n",
    "        # find closest filepath based on timestamp\n",
    "        filename, ct_k = find_closest_filename(data_frames[j], t_u)\n",
    "\n",
    "        image = cv2.imread(f'{data_frames[j]}/{filename}')\n",
    "        \n",
    "        # ct_k = int(filename.replace('frame_', '').replace('.jpg', '')) # timestamp\n",
    "        ct_frame = 30\n",
    "\n",
    "        if t_u - ct_k < 24:\n",
    "            my_roi = roi(t_u, ct_k, ct_frame, image)\n",
    "\n",
    "            roi_image = image[:, int(my_roi[0]):int(my_roi[1])]\n",
    "\n",
    "            u = roi_image.shape[0] * colorChanges.iloc[i,2]  # redefine u?\n",
    "            target = u + 0.1 # u + d / 2\n",
    "\n",
    "            rois.append(roi_image)\n",
    "            targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"DATA\", len(rois), len(targets))\n",
    "# split_index = 300\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1]), array([360.1])]\n",
      "SCORE 57610.64610169492\n"
     ]
    }
   ],
   "source": [
    "# train_x, test_x = rois[:split_index], rois[split_index:]\n",
    "# train_y, test_y = targets[:split_index], targets[split_index:]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(rois, targets, test_size=0.33)\n",
    "lr_model = lr_fit(train_x, train_y)\n",
    "predictions, score = lr_predict(lr_model, test_x, test_y)\n",
    "\n",
    "print(predictions)\n",
    "print(\"SCORE\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
