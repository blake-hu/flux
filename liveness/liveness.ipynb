{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images - line by line, cmaera - col by col\n",
    "    Q: is this hardware specific?\n",
    "screen waits 1 frame cycle, camera waits 1 frame cycle \n",
    "    Q: r they the same?\n",
    "\n",
    "\n",
    "two kinds of challenges:\n",
    "    background challenge: one color\n",
    "    lighting challenge: belt of different colr from background color\n",
    "        belt is \"lighting area\"\n",
    "\n",
    "ROI: region that camera is scanning when the screen is displaying the lighting area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "from sklearn.linear_model import LinearRegression  \n",
    "import pandas as pd\n",
    "from deepface import DeepFace\n",
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = \"blake_video.mp4\" # Replace with path of input video\n",
    "output_directory1 = \"output_1\"\n",
    "frames = \"videoframes_raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS 30.011996453222526\n",
      "Video frames saved in 'output_1'\n"
     ]
    }
   ],
   "source": [
    "# Split video into frames in the videoframes directory\n",
    "# Runtime: ~60 seconds\n",
    "\n",
    "# Delete directory if it already exists and make a new one\n",
    "if os.path.exists(output_directory1):\n",
    "   shutil.rmtree(output_directory1)\n",
    "os.makedirs(output_directory1)\n",
    "\n",
    "vidcap = cv2.VideoCapture(input_video)\n",
    "fps = vidcap.get(cv2.CAP_PROP_FPS) # get fps\n",
    "frametime_list = []\n",
    "print(\"FPS\", fps)\n",
    "\n",
    "if vidcap.isOpened():\n",
    "  while True:\n",
    "      ret, frame = vidcap.read()\n",
    "      \n",
    "      if not ret:\n",
    "          break\n",
    "      \n",
    "      # Get the timestamp of the current frame in milliseconds -> TODO: currently unused \n",
    "      timestamp = round(vidcap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "      frametime_list.append(timestamp)\n",
    "      \n",
    "      # Save each frame as an image in the output folder\n",
    "      frame_name = f\"frame_{timestamp}.jpg\"\n",
    "      frame_path = os.path.join(output_directory1, frame_name)\n",
    "      cv2.imwrite(frame_path, frame)\n",
    "\n",
    "  vidcap.release()\n",
    "\n",
    "  frametime_list = np.array(frametime_list)\n",
    "\n",
    "  print(f\"Video frames saved in '{output_directory1}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT FRAMES INTO CROPPED FRAMES \n",
    "if os.path.exists(frames):\n",
    "   shutil.rmtree(frames)\n",
    "os.makedirs(frames)\n",
    "\n",
    "for filename in sorted(os.listdir(output_directory1)):\n",
    "    frame_path = os.path.join(output_directory1, filename)\n",
    "    \n",
    "    if os.path.isfile(frame_path):  # Check if it's a file (not a subdirectory)\n",
    "        #face detection and alignment\n",
    "        img = cv2.imread(frame_path)\n",
    "\n",
    "        try: #skip frame if can't detect face\n",
    "            face_objs = DeepFace.extract_faces(img_path = frame_path, \n",
    "                    target_size = (224, 224)\n",
    "            )\n",
    "        except:\n",
    "            print(\"Couldn't detect face in frame \" + frame_path)\n",
    "\n",
    "        face = face_objs[0]['facial_area']\n",
    "        x,y,w,h = face['x'],face['y'],face['w'],face['h']\n",
    "\n",
    "        img = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Save each frame as an image in the output folder\n",
    "        frame_name = f\"frame_{frame_count:04d}.jpg\"\n",
    "        frame_path = os.path.join(frames, filename)\n",
    "        cv2.imwrite(frame_path, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eqn 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eqn_2(pixel, color1, color2, E):\n",
    "    '''\n",
    "    #INPUTS:\n",
    "        pixel: a single pixel with all 3 color channels\n",
    "        color1: background color being shown on screen\n",
    "        color2: primary color being shown on screen (band)\n",
    "        E: illuminance for all 3 channels \n",
    "\n",
    "    Confirm that I{c1}/I{c2} = E{c1}/E{c2} (where c1 and c2 are the 2 colors being shown on the screen)  \n",
    "    '''\n",
    "    iFraction = pixel[color1]/(pixel[color2]+1)\n",
    "    eFraction = E[color1]/E[color2]\n",
    "    epsilon = 0.01\n",
    "    return iFraction- eFraction <= epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyEqn2(color1, color2, img):\n",
    "    # Apply Eqn 2 on every pixel between response of lighting challenge and background challenge\n",
    "    count = 0\n",
    "    E = [0,0,0]\n",
    "    E[int(color1)] = 256\n",
    "    E[int(color2)] = 256\n",
    "    for r in range(img.shape[0]):\n",
    "        for c in range(img.shape[1]):\n",
    "            consistent = eqn_2(img[r][c][:], color1, color2, E)\n",
    "            if not consistent:\n",
    "                count+=1\n",
    "                # print(\"Not consistent!\")\n",
    "                # return\n",
    "\n",
    "    return (count/(img.shape[0]*img.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02408051639888194\n"
     ]
    }
   ],
   "source": [
    "#EQN2 CHECK:\n",
    "image = cv2.imread('videoframes_raw/frame_0.jpg')\n",
    "color1 = 1\n",
    "color2 = 2\n",
    "E = [0,256,256]\n",
    "inconsistent = verifyEqn2(color1, color2, image)\n",
    "print(inconsistent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 30 #(camera)\n",
    "# 1/120 ms to draw one frame on the screen\n",
    "def roi(t_u, ct_k, ct_frame, image):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        t_u = time that this color started\n",
    "        u = top of band\n",
    "        ct_k = start time to exposure the first column of k-th capture frame\n",
    "            --> find w/ firstImg\n",
    "        ct_frame = exposure time of one captured frame \n",
    "            --> average time of each frame ? maybe can calculate w/ dict. \n",
    "        image = first image whose recording period covers t_u\n",
    "    '''\n",
    "    cols = image.shape[1]\n",
    "    a = cols * (t_u - ct_k)/ct_frame\n",
    "    b = a+0.2*image.shape[1]\n",
    "    return [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Background Color</th>\n",
       "      <th>Strip Color</th>\n",
       "      <th>Strip Position</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Background Color  Strip Color  Strip Position   Timestamp\n",
       "0                1            2             0.3           0\n",
       "1                0            2             0.4         501\n",
       "2                0            1             0.4        1002\n",
       "3                0            2             0.7        1501\n",
       "4                1            2             0.1        2002"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapColors(color):\n",
    "    if color == \"Red\":\n",
    "        return 0\n",
    "    if color == \"Green\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def removePercent(s):\n",
    "    return float(\"0.\" + s[:-1])\n",
    "\n",
    "colorChanges = pd.read_csv('colors.csv')\n",
    "colorChanges.iloc[:,0] = colorChanges.iloc[:,0].apply(mapColors)\n",
    "colorChanges.iloc[:,1] = colorChanges.iloc[:,1].apply(mapColors)\n",
    "colorChanges.iloc[:,2] = colorChanges.iloc[:,2].apply(removePercent)\n",
    "colorChanges.iloc[:,3]= colorChanges.iloc[:,3]-colorChanges.iloc[0,3]\n",
    "\n",
    "colorChanges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "videoframes_raw/frame_0.jpg\n",
      "(446, 446, 3)\n",
      "inconsistency:  0.02408051639888194\n",
      "a, b:  178 268\n",
      "\n",
      "videoframes_raw/frame_472.jpg\n",
      "(457, 457, 3)\n",
      "inconsistency:  0.0011587319067843274\n",
      "a, b:  42 133\n",
      "\n",
      "videoframes_raw/frame_1005.jpg\n",
      "(452, 452, 3)\n",
      "inconsistency:  0.002745908058579372\n",
      "a, b:  55 145\n",
      "\n",
      "videoframes_raw/frame_1472.jpg\n",
      "(452, 452, 3)\n",
      "inconsistency:  0.0011453520244341766\n",
      "a, b:  41 131\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): #for each color change\n",
    "    color1, color2 = colorChanges.iloc[i,0],colorChanges.iloc[i,1]\n",
    "    u, startTime = colorChanges.iloc[i,2], colorChanges.iloc[i,3]\n",
    "    offset = random.randint(2,6)# randomly select a ms offset \n",
    "    t_u = startTime+offset\n",
    "    print()\n",
    "    \n",
    "    imgIndex = np.sum(frametime_list<t_u)-1 #find all frames capturing time less than t_u, select last one\n",
    "    imgPath = 'videoframes_raw/frame_' + str(frametime_list[imgIndex]) + \".jpg\"\n",
    "    print(imgPath)\n",
    "    img = cv2.imread(imgPath)\n",
    "    print(img.shape)\n",
    "\n",
    "    exposureTime =  frametime_list[imgIndex+1]-frametime_list[imgIndex]\n",
    "\n",
    "    #calculate eqn2 \n",
    "    inconsistency = verifyEqn2(color1, color2, img)\n",
    "    print(\"inconsistency: \", inconsistency)\n",
    "    if inconsistency >0.1:     #STOP HERE IF DOESNT PASS\n",
    "        print(\"INCONSISTENT\")\n",
    "        break\n",
    "\n",
    "    a,b = roi(t_u, startTime, exposureTime, img)\n",
    "    a = round(a)\n",
    "    b = round(b)\n",
    "    print(\"a, b: \", a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models():\n",
    "    # regression model\n",
    "    a,b=10,50\n",
    "    ROI = (a,b)\n",
    "    \n",
    "    imgWidth = image.shape[1]\n",
    "\n",
    "    y_hat = 0\n",
    "    if a < 0.25*imgWidth:\n",
    "        avgCols = np.mean(image[:, a:25, :]) # take average value of columns [a,25] \n",
    "                ## ACROSS RGB ??\n",
    "        output = lr1.predict(avgCols) #run regression on model 1\n",
    "        y_hat += (0.25*imgWidth-a)*output\n",
    "    if b < 0.5*imgWidth:\n",
    "        #model2: avg. value of columns [25, b]\n",
    "        # run regression on model 2 \n",
    "        avgCols = None \n",
    "        output = lr2.predict(avgCols)\n",
    "        y_hat += (b-0.25*imgWidth)*output\n",
    "\n",
    "    if b < 0.75*imgWidth and a > 0.25*imgWidth:\n",
    "        #model 3: avg value of columns [50,b]\n",
    "        avgCols = None\n",
    "        output = lr3.predict(avgCols)\n",
    "        y_hat += (b-0.5*imgWidth)*output\n",
    "\n",
    "    if b >=0.75*imgWidth:\n",
    "        #model 4 : avg value of columns [75,b]\n",
    "        avgCols = None\n",
    "        output = lr4.predict(avgCols)\n",
    "        y_hat += (b-0.75*imgWidth)*output\n",
    "    \n",
    "    y_hat = y_hat /imgWidth\n",
    "\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Calculate ROI\n",
    "# calculate start time to show the lighting area, t_u\n",
    "t_u = 0 #(u / rows) * t_frame\n",
    "# find image - frame at t_u\n",
    "img = frames[0] #[fps * t_u]\n",
    "\n",
    "# calculate shift, l\n",
    "l = 0 #cols * (t_u - ct_k) / (ct_frame)\n",
    "\n",
    "u = 0.75 * img.shape[0]\n",
    "rows = 0\n",
    "t_frame = 0\n",
    "cols = 0\n",
    "ct_k = 0\n",
    "ct_frame = 0\n",
    "fps = 0\n",
    "color1 = 0\n",
    "color2 = 1\n",
    "E = [234,251,0]\n",
    "\n",
    "y_hat = roi(u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criteria(frames):\n",
    "    t_begin, u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E = 1,2,3,4,5,6,7,8,9,10,11\n",
    "    \n",
    "    dVals = []\n",
    "    for frame in frames:\n",
    "        y_hat_i = roi(frames, t_begin, u, rows, t_frame, cols, ct_k, ct_frame, fps, color1, color2, E)\n",
    "\n",
    "        # calc criteria\n",
    "        d = 0.25 #% of screen the band is shown on \n",
    "        imgRows = frame.shape[0] \n",
    "        \n",
    "        d_i = y_hat_i - (u + u+imgRows*0.25)/2\n",
    "        dVals.append(d_i)\n",
    "\n",
    "    mean = np.mean(dVals)\n",
    "    var = np.var(dVals)\n",
    "\n",
    "    threshold = -5\n",
    "    return mean * np.sqrt(var) < np.exp(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training linear regression models to be used to identify location of band of color\n",
    "def get_average_vector(roi):\n",
    "    # Calculate the mean along the height and width (axis 0 and 1), resulting in mean color\n",
    "    return np.mean(roi, axis=(0, 1))\n",
    "\n",
    "def linear_regression(rois, targets):\n",
    "    predictions = []\n",
    "    for i in range(len(rois)):\n",
    "        avg_vector = get_average_vector(rois[i])\n",
    "        lr = LinearRegression().fit([avg_vector], [targets[i]])\n",
    "        prediction = lr.predict([avg_vector])\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "image = cv2.imread('videoframes_raw/frame_505.jpg')\n",
    "t_u = 3\n",
    "ct_k = 1\n",
    "ct_frame = 30\n",
    "roi1 = roi(t_u, ct_k, ct_frame, image)\n",
    "\n",
    "# print(\"ROI\", roi1)\n",
    "roi_image = image[:, int(roi1[0]):int(roi1[1])]\n",
    "\n",
    "# print(roi_image.shape)\n",
    "\n",
    "u = roi_image.shape[0] * colorChanges.iloc[1,2] #(corresponds to 501 timestamp)\n",
    "# print(colorChanges.iloc[1,2]) # corresponds to 501 timestamp\n",
    "\n",
    "target = u + 0.1 # (u + (u+0.2)) / 2\n",
    "\n",
    "rois = [roi_image]\n",
    "targets = [target]\n",
    "predictions = linear_regression(rois, targets)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
